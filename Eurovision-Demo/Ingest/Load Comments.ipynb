{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4092a4f-40d8-41ee-abdb-576483aea3a0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb3eeb8-f42c-420d-a342-f092cb1a1351",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "session_starting",
       "parent_msg_id": "7627520b-fd8d-4eb7-92e3-3852357f39f7",
       "queued_time": "2025-04-28T19:54:28.1709286Z",
       "session_id": null,
       "session_start_time": "2025-04-28T19:54:28.1718931Z",
       "spark_pool": null,
       "state": "session_starting",
       "statement_id": -1,
       "statement_ids": []
      },
      "text/plain": [
       "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "\n",
    "from delta import DeltaTable\n",
    "from typing import Any, List, Tuple, Optional\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae26263-2036-4113-8d1a-e00d6fec580b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:25.9728397Z",
       "execution_start_time": "2025-03-16T22:27:25.9189614Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "651e7426-116d-4686-b0c2-b70fbad81e7c",
       "queued_time": "2025-03-16T22:27:24.6529555Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 19,
       "statement_ids": [
        17,
        18,
        19
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 19, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Google-Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c4929-fb39-4182-9b68-12177f75d1e5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:26.293962Z",
       "execution_start_time": "2025-03-16T22:27:25.9752581Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d7f34ea1-ef84-48da-b7d6-8db0c35b7b47",
       "queued_time": "2025-03-16T22:27:24.8764733Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 20,
       "statement_ids": [
        20
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 20, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Youtube API\n",
    "API_SERVICE_NAME = \"youtube\"\n",
    "API_VERSION = \"v3\"\n",
    "DEVELOPER_KEY = \"\"\n",
    "\n",
    "# Delta table names\n",
    "VIDEOS_TABLE = \"abfss://fd12376e-2797-4027-bb8e-42a3a8228a70@onelake.dfs.fabric.microsoft.com/77b89b44-1bcf-42fa-a9ac-7d0593123d3d/Tables/videos\"\n",
    "COMMENTS_TABLE = \"abfss://fd12376e-2797-4027-bb8e-42a3a8228a70@onelake.dfs.fabric.microsoft.com/77b89b44-1bcf-42fa-a9ac-7d0593123d3d/Tables/comments\" \n",
    "TEMP_TABLE = \"abfss://fd12376e-2797-4027-bb8e-42a3a8228a70@onelake.dfs.fabric.microsoft.com/77b89b44-1bcf-42fa-a9ac-7d0593123d3d/Tables/temp_comments\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f85780-4b68-494d-93ca-0e6274f7b94c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:26.600725Z",
       "execution_start_time": "2025-03-16T22:27:26.2963807Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "adda72ca-b99f-4022-add5-450dfd0d2889",
       "queued_time": "2025-03-16T22:27:24.8779078Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 21,
       "statement_ids": [
        21
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 21, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 22:27:26,424 - INFO - Building YouTube API client\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logger()\n",
    "youtube_client = build_youtube_client(API_SERVICE_NAME,API_VERSION,DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a38cb-949b-42be-9168-dcbe4b190aa5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30ecba8f-44ed-4f81-9dfe-6ab4f2f02080",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:26.8925052Z",
       "execution_start_time": "2025-03-16T22:27:26.6035983Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6ed7ca9a-f482-48ef-ae38-83936e87fe96",
       "queued_time": "2025-03-16T22:27:24.8971797Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 22,
       "statement_ids": [
        22
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 22, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_comments_for_video_partition(rows_iterator: RDD, comment_count: int = 100, batch_size: int = 100) -> List:\n",
    "    \"\"\"\n",
    "    Fetch the comments for a video in batches up to the comment_count.\n",
    "    It creates one YouTube client per partition, required to ensure no bottlenecks on the API client.\n",
    "    \n",
    "    Args:\n",
    "        rows_iterator: Partition of video Ids\n",
    "        comment_count: Max number of comments to return\n",
    "        batch_size: Size of API batches\n",
    "\n",
    "    Yields:\n",
    "        Comments for the partition\n",
    "    \"\"\"\n",
    "    youtube_client = build_youtube_client(API_SERVICE_NAME,API_VERSION,DEVELOPER_KEY)\n",
    "    for video_id in rows_iterator:\n",
    "        comments = fetch_comments_for_video_with_client(video_id, youtube_client, comment_count, batch_size)\n",
    "        # Ensure 'comments' is always iterable.\n",
    "        if comments is None:\n",
    "            logger.warning(f\"No comments returned for video {video_id}. Skipping.\")\n",
    "            continue\n",
    "        for comment in comments:\n",
    "            yield comment\n",
    "\n",
    "def fetch_comments_for_video_with_client(video_id: str, youtube_client: googleapiclient.discovery.Resource, comment_count: int = 100, batch_size: int = 100) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Fetch the comments for a video in batches up to the comment_count.\n",
    "    \n",
    "    Args:\n",
    "        video_id: Id for video \n",
    "        youtube_client: Youtube API resource object\n",
    "        comment_count: Max number of comments to return\n",
    "        batch_size: Size of API batches, max of 100\n",
    "\n",
    "    Returns:\n",
    "        A list of comments\n",
    "    \"\"\"\n",
    "    rows: List[Tuple] = []\n",
    "    if batch_size > 100:\n",
    "        logger.warning(\"Batch over 100, setting to 100\")\n",
    "        batch_size = 100\n",
    "    \n",
    "    try:\n",
    "        request = youtube_client.commentThreads().list(\n",
    "            part=\"id,snippet\",\n",
    "            maxResults=batch_size,\n",
    "            moderationStatus=\"published\",\n",
    "            order=\"relevance\",\n",
    "            videoId=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for d in response.get(\"items\", []):\n",
    "            row = (\n",
    "                d[\"id\"],\n",
    "                d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"],\n",
    "                d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
    "                d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"],\n",
    "                video_id,\n",
    "            )\n",
    "            rows.append(row)\n",
    "        \n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        while next_page_token and len(rows) < comment_count:\n",
    "            request = youtube_client.commentThreads().list(\n",
    "                part=\"id,snippet\",\n",
    "                maxResults=batch_size,\n",
    "                moderationStatus=\"published\",\n",
    "                order=\"relevance\",\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "            for d in response.get(\"items\", []):\n",
    "                row = (\n",
    "                    d[\"id\"],\n",
    "                    d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"],\n",
    "                    d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
    "                    d[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"],\n",
    "                    video_id,\n",
    "                )\n",
    "                rows.append(row)\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "    except HttpError as httpe:\n",
    "        # When the video is not found, a 404 error is usually thrown.\n",
    "        if err.resp.status == 404:\n",
    "            logger.warning(f\"Video {video_id} not found. Skipping.\")\n",
    "            return []\n",
    "        else:\n",
    "            logger.exception(f\"HTTP error fetching comments for video {video_id}: {str(httpe)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error fetching comments for video {video_id}: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3821c7f9-b34f-4f8f-867e-9c8a8bc8e6ab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:27.1629377Z",
       "execution_start_time": "2025-03-16T22:27:26.8953023Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "433dd7a5-f4ac-405d-9dcd-3e5ac537dd46",
       "queued_time": "2025-03-16T22:27:24.9542184Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 23,
       "statement_ids": [
        23
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 23, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_video_ids(video_abfs: str, new_videos_only: bool = False, comments_abfs: Optional[str] = None) -> RDD:\n",
    "    \"\"\"\n",
    "    Fetch video IDs from a Delta table located at the given ABFS path. Optionally, return only those videos that do not appear in the comments \n",
    "    \n",
    "    Args:\n",
    "        video_abfs: ABFS path for the videos table.\n",
    "        new_videos_only: If True, only returns videos with no associated comments.\n",
    "        comments_abfs: ABFS path for the comments table. Required when `new_videos_only` is True.\n",
    "    \n",
    "    Returns:\n",
    "        An RDD containing the video IDs.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If `new_videos_only` is True but `comments_abfs` is not provided.\n",
    "    \"\"\"\n",
    "    if (new_videos_only and comments_abfs is None):\n",
    "        message = \"No comments_abfs specified while new_videos_only is True.\"\n",
    "        logger.error(message)\n",
    "        raise ValueError(message)\n",
    "\n",
    "    logger.info(\"Reading video IDs from Raw.videos table\")\n",
    "    video_ids = spark.read.format(\"delta\").load(video_abfs).select(\"id\")\n",
    "\n",
    "    if new_videos_only:\n",
    "        logger.debug(\"Filtering to only new videos\")\n",
    "        comment_video_ids = spark.read.format(\"delta\").load(comments_abfs).select(\"videoId\")\n",
    "        # Use left anti join to retain only those video IDs not present in the comments.\n",
    "        video_ids = video_ids.join(comments_df, video_ids.id == comments_df.videoId, \"left_anti\")\n",
    "\n",
    "    if video_ids.isEmpty():\n",
    "        logger.info(f\"Read in 0 video IDs\")\n",
    "        return spark.sparkContext.emptyRDD()\n",
    "    else:\n",
    "        logger.info(f\"Read in {video_ids.count()} video IDs\")\n",
    "        return video_ids.rdd.map(lambda row: row.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c11966-9160-49ba-b22f-269f2a8bb54c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:38.6613684Z",
       "execution_start_time": "2025-03-16T22:27:27.165521Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "613bcd49-bec1-4ecd-bd1b-085411e07246",
       "queued_time": "2025-03-16T22:27:25.1251704Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 24,
       "statement_ids": [
        24
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 24, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 22:27:27,251 - INFO - Reading video IDs from Raw.videos table\n",
      "2025-03-16 22:27:36,559 - INFO - Read in 227 video IDs\n"
     ]
    }
   ],
   "source": [
    "video_ids_rdd = fetch_video_ids(VIDEOS_TABLE, new_videos_only=False,comments_abfs=COMMENTS_TABLE)\n",
    "\n",
    "if video_ids_rdd.isEmpty():\n",
    "    message = \"No video IDs found. Exiting.\"\n",
    "    logger.info(message)\n",
    "    notebookutils.notebook.exit(message)\n",
    "\n",
    "# Use mapPartitions to process a batch of video IDs with one YouTube client per partition.\n",
    "comments_rdd = video_ids_rdd.mapPartitions(lambda rows: fetch_comments_for_video_partition(rows, comment_count=100, batch_size=100))\n",
    "\n",
    "# Define the schema for the comments DataFrame.\n",
    "comments_schema = T.StructType([\n",
    "    T.StructField(\"id\", T.StringType(), True),\n",
    "    T.StructField(\"textDisplay\", T.StringType(), True),\n",
    "    T.StructField(\"publishedAt\", T.StringType(), True),\n",
    "    T.StructField(\"likeCount\", T.StringType(), True),\n",
    "    T.StructField(\"videoId\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "# Create a DataFrame from the RDD.\n",
    "comments_df = spark.createDataFrame(comments_rdd, schema=comments_schema)\n",
    "\n",
    "# Convert types\n",
    "comments_df = comments_df.withColumn(\n",
    "        \"publishedAt\", \n",
    "        F.to_timestamp(\"publishedAt\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "    ).withColumn(\n",
    "        \"likeCount\",\n",
    "        comments_df.likeCount.cast(T.IntegerType())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cbd6b5-eec3-4b81-9c58-abc907e3dd58",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "594f6bac-4061-4b24-a06e-0d0daf871d02",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:27:38.9501417Z",
       "execution_start_time": "2025-03-16T22:27:38.6648711Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "20038567-5519-4367-a47e-aed7f25f282f",
       "queued_time": "2025-03-16T22:27:25.1836477Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 25,
       "statement_ids": [
        25
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 25, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_comments_data(comments_df: DataFrame, table_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Merge the comments data into the Delta table\n",
    "    \n",
    "    Args:\n",
    "        comments_df: The DataFrame containing the new video data and details.\n",
    "        table_path: The Delta table ABFS path to merge into.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_table = DeltaTable.forPath(spark, table_path)\n",
    "        logger.info(\"Merging data started\")\n",
    "        (\n",
    "            target_table.alias(\"target\").merge(\n",
    "                comments_df.alias(\"source\"),\n",
    "                \"target.id = source.id\"\n",
    "            ).whenMatchedUpdate(set={\n",
    "                \"textDisplay\": \"source.textDisplay\",\n",
    "                \"publishedAt\": \"source.publishedAt\",\n",
    "                \"likeCount\": \"source.likeCount\",\n",
    "                \"videoId\": \"source.videoId\",\n",
    "                \"_modified_date\": \"current_timestamp()\"\n",
    "            })\n",
    "            .whenNotMatchedInsert(values={\n",
    "                \"id\": \"source.id\",\n",
    "                \"textDisplay\": \"source.textDisplay\",\n",
    "                \"publishedAt\": \"source.publishedAt\",\n",
    "                \"likeCount\": \"source.likeCount\",\n",
    "                \"videoId\": \"source.videoId\",\n",
    "                \"_created_date\": \"current_timestamp()\",\n",
    "                \"_modified_date\": \"current_timestamp()\"\n",
    "            })\n",
    "            .execute()\n",
    "        )\n",
    "        logger.info(\"Merging data finished\")\n",
    "        lastCommit = target_table.history(1).collect()[0]\n",
    "        metrics = lastCommit[\"operationMetrics\"] \n",
    "\n",
    "        numInserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "        numUpdated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "        numDeleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "\n",
    "        logger.info(f\"Rows inserted: {numInserted}\")\n",
    "        logger.info(f\"Rows updated: {numUpdated}\")\n",
    "        logger.info(f\"Rows deleted: {numDeleted}\")\n",
    "        try:\n",
    "            logger.info(\"Start optimize\")\n",
    "            target_table.optimize().executeCompaction()\n",
    "            logger.info(\"Finished optimize\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to optimize\")\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "092c8235-adbb-4f61-b5ba-d1b23ee630b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:28:56.8051362Z",
       "execution_start_time": "2025-03-16T22:27:38.9527001Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "80d4b2f3-4748-4feb-9b02-881fd1f1ddc1",
       "queued_time": "2025-03-16T22:27:25.3643929Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 26,
       "statement_ids": [
        26
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 26, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 22:27:39,054 - INFO - Writing out comments to temp table\n",
      "2025-03-16 22:28:55,848 - INFO - Finished writing temp table: 22700 rows\n"
     ]
    }
   ],
   "source": [
    "# Writing to temp delta table to reduce repeat API calls on failures and improve merge performance\n",
    "try:\n",
    "    logger.info(\"Writing out comments to temp table\")\n",
    "    comments_df.write.format(\"delta\").mode(\"overwrite\").save(TEMP_TABLE)\n",
    "    comments_delta = spark.read.format(\"delta\").load(TEMP_TABLE)\n",
    "    logger.info(f\"Finished writing temp table: {comments_delta.count()} rows\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error writing to temporary Delta table at {TEMP_TABLE}: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0821dffa-2cf9-413c-8b42-6fd7dcf22285",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:29:10.6965547Z",
       "execution_start_time": "2025-03-16T22:28:56.8082151Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "75d0ac33-fdca-4d34-bcac-d448062a0de3",
       "queued_time": "2025-03-16T22:27:25.3662366Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 27,
       "statement_ids": [
        27
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 27, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 22:28:58,235 - INFO - Merging data started\n",
      "2025-03-16 22:29:05,677 - INFO - Merging data finished\n",
      "2025-03-16 22:29:06,447 - INFO - Rows inserted: 5984\n",
      "2025-03-16 22:29:06,448 - INFO - Rows updated: 16716\n",
      "2025-03-16 22:29:06,448 - INFO - Rows deleted: 0\n",
      "2025-03-16 22:29:06,449 - INFO - Start optimize\n",
      "2025-03-16 22:29:09,773 - INFO - Finished optimize\n"
     ]
    }
   ],
   "source": [
    "merge_comments_data(comments_delta, COMMENTS_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf9f38-dc9d-443f-91b8-1cd8f96d83e4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Cleanup temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a283921-7c1e-436b-978b-e227448a128d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:29:10.9954398Z",
       "execution_start_time": "2025-03-16T22:29:10.699762Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "528f1a13-7847-4d54-b700-a2773580f81f",
       "queued_time": "2025-03-16T22:27:25.3675594Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 28,
       "statement_ids": [
        28
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 28, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cleanup_delta_staging_table(temp_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Cleans up a staging Delta table.\n",
    "    \n",
    "    Args:\n",
    "        temp_path (str): The Azure Data Lake path (abfss:// URI) of the staging Delta table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get details for the staging table.\n",
    "        directory, table_name = temp_path.rsplit('/', 1)\n",
    "        all_databases = spark.catalog.listDatabases()\n",
    "        database = [t for t in all_databases if t.locationUri == directory][0].name\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {database}.`{table_name}`\")\n",
    "        logger.info(\"Dropped temp table\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Warning: Could not delete Delta table at {database}.`{table_name}`. Error: {str(e)}\")\n",
    "        raise\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab3176e-4210-487f-b6e9-40903bde93f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-16T22:29:26.7630387Z",
       "execution_start_time": "2025-03-16T22:29:10.9981202Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6faa38b1-c2fe-4dd0-94ac-f6f1365f9494",
       "queued_time": "2025-03-16T22:27:25.4173722Z",
       "session_id": "155b6c3c-1030-40e0-8c51-5cc53b535e52",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 29,
       "statement_ids": [
        29
       ]
      },
      "text/plain": [
       "StatementMeta(, 155b6c3c-1030-40e0-8c51-5cc53b535e52, 29, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 22:29:26,116 - INFO - Dropped temp table\n"
     ]
    }
   ],
   "source": [
    "cleanup_delta_staging_table(TEMP_TABLE)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "143044df-8d0d-4272-ae18-32804b812e76",
    "workspaceId": "fd12376e-2797-4027-bb8e-42a3a8228a70"
   },
   "lakehouse": {
    "default_lakehouse": "77b89b44-1bcf-42fa-a9ac-7d0593123d3d",
    "default_lakehouse_name": "Raw",
    "default_lakehouse_workspace_id": "fd12376e-2797-4027-bb8e-42a3a8228a70"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
