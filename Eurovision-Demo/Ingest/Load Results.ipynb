{"cells":[{"cell_type":"markdown","source":["### Setup"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c85c6c7-a1a7-423c-b722-b3ba5e6dc49c"},{"cell_type":"code","source":["from typing import Tuple, Optional\n","import logging\n","import requests\n","from bs4 import BeautifulSoup\n","from io import StringIO\n","import pyspark.pandas as ps\n","import concurrent.futures\n","from datetime import datetime\n","\n","from delta.tables import DeltaTable\n","\n","from pyspark.sql import SparkSession, DataFrame\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.5547599Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:36.5559156Z","execution_finish_time":"2025-04-28T21:00:36.8712402Z","parent_msg_id":"cce36009-1a89-4963-a7fe-0806d19c9765"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 26, Finished, Available, Finished)"},"metadata":{}}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04a576d7-5b71-420a-8adf-362cb073f9d9"},{"cell_type":"code","source":["%run Helpers"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[27,28],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.6649781Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:37.5679242Z","execution_finish_time":"2025-04-28T21:00:37.6270291Z","parent_msg_id":"054c7178-c923-4f98-ba9a-d347395796aa"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 28, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cdd7728c-ed95-45e8-b859-ee8652eb0697"},{"cell_type":"code","source":["logger = setup_logger()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.7358167Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:37.6286177Z","execution_finish_time":"2025-04-28T21:00:37.9440918Z","parent_msg_id":"335b49ea-f861-4254-98fd-e1b677f095cb"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 29, Finished, Available, Finished)"},"metadata":{}}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fec2890-0b80-4082-8789-50d53d4f42b3"},{"cell_type":"code","source":["PLAYLISTS_TABLE = \"abfss://fd12376e-2797-4027-bb8e-42a3a8228a70@onelake.dfs.fabric.microsoft.com/77b89b44-1bcf-42fa-a9ac-7d0593123d3d/Tables/playlists\"\n","RESULTS_TABLE = \"abfss://fd12376e-2797-4027-bb8e-42a3a8228a70@onelake.dfs.fabric.microsoft.com/77b89b44-1bcf-42fa-a9ac-7d0593123d3d/Tables/results\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":30,"statement_ids":[30],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.9018266Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:37.9461854Z","execution_finish_time":"2025-04-28T21:00:38.2443733Z","parent_msg_id":"bf690df1-6da4-4ab7-94a4-1da86a38c33b"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 30, Finished, Available, Finished)"},"metadata":{}}],"execution_count":24,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2fded74e-cd8d-41f6-835f-f6840ef7d718"},{"cell_type":"markdown","source":["### Load data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b27c22f-6395-4cc2-ae61-f052836022cc"},{"cell_type":"code","source":["def has_expected_columns(table) -> bool:\n","    \"\"\"\n","    Check if the BeautifulSoup Tag table has the essential columns:\n","    \"R/O\", \"Country\", \"Artist\", \"Song\", \"Points\", \"Place\".\n","    \"\"\"\n","    header_row = table.find(\"tr\")\n","    if not header_row:\n","        return False\n","    headers = {th.get_text(strip=True) for th in header_row.find_all(\"th\")}\n","    required = {\"R/O\",\"Country\", \"Artist\", \"Song\", \"Points\",\"Place\"}\n","    if not required.issubset(headers):\n","        return False\n","    return True\n","\n","def fetch_year_page(year: int) -> BeautifulSoup:\n","    \"\"\"\n","    Fetch the Wikipedia page for the given Eurovision year\n","    and return a BeautifulSoup object.\n","    \"\"\"\n","    url = f\"https://en.wikipedia.org/wiki/Eurovision_Song_Contest_{year}#Contest_overview\"\n","    headers_req = {\"User-Agent\": \"Mozilla/5.0\"}\n","    logger.info(f\"Year {year}: Fetching URL {url}\")\n","    try:\n","        response = requests.get(url, headers=headers_req)\n","        response.raise_for_status()\n","        return BeautifulSoup(response.text, \"html.parser\")\n","    except Exception as e:\n","        logger.error(f\"Year {year}: Error fetching URL: {e}\")\n","        raise\n","\n","def extract_table_htmls(soup: BeautifulSoup, year: int) -> Tuple[str, str, str]:\n","    \"\"\"\n","    Iterate through wiki tables in the parsed soup and\n","    extract the HTML for:\n","      - Final table,\n","      - Semi-final 1 table,\n","      - Semi-final 2 table.\n","    Only tables whose captions contain expected keywords and whose header rows\n","    have the required columns are accepted.\n","    \n","    Returns:\n","        A tuple (final_table_html, semifinal1_table_html, semifinal2_table_html)\n","    \"\"\"\n","    final_table_html = None\n","    semifinal1_table_html = None\n","    semifinal2_table_html = None\n","    \n","    for table in soup.find_all(\"table\", class_=\"wikitable\"):\n","        caption_tag = table.find(\"caption\")\n","        if not caption_tag:\n","            continue\n","        caption_text = caption_tag.get_text(strip=True).lower()\n","        if not has_expected_columns(table):\n","            logger.debug(f\"Year {year}: Skipping table with caption '{caption_text}' due to missing expected columns.\")\n","            continue\n","        if f\"final of the eurovision song contest {year}\" in caption_text and \"semi\" not in caption_text:\n","            final_table_html = str(table)\n","            logger.info(f\"Year {year}: Final table found with caption: {caption_text}\")\n","        elif f\"first semi-final of the eurovision song contest {year}\" in caption_text:\n","            semifinal1_table_html = str(table)\n","            logger.info(f\"Year {year}: Semi-final 1 table found with caption: {caption_text}\")\n","        elif f\"second semi-final of the eurovision song contest {year}\" in caption_text:\n","            semifinal2_table_html = str(table)\n","            logger.info(f\"Year {year}: Semi-final 2 table found with caption: {caption_text}\")\n","            \n","    if not final_table_html:\n","        logger.error(f\"Year {year}: Final table not found by caption filtering.\")\n","        raise ValueError(f\"Final table not found for year {year}.\")\n","    if not semifinal1_table_html:\n","        logger.error(f\"Year {year}: Semi-final 1 table not found by caption filtering.\")\n","        raise ValueError(f\"Semi-final 1 table not found for year {year}.\")\n","    if not semifinal2_table_html:\n","        logger.error(f\"Year {year}: Semi-final 2 table not found by caption filtering.\")\n","        raise ValueError(f\"Semi-final 2 table not found for year {year}.\")\n","        \n","    return final_table_html, semifinal1_table_html, semifinal2_table_html\n","\n","def load_and_standardize_tables(final_table_html: str, semifinal1_table_html: str, semifinal2_table_html: str) -> Tuple[DataFrame, DataFrame, DataFrame]:\n","    \"\"\"\n","    Read the HTML tables (as strings) using pyspark.pandas.read_html,\n","    reset indices, rename columns to standard names, and convert to Spark DataFrames.\n","    \n","    Returns:\n","        A tuple (final_df, semifinal1_df, semifinal2_df)\n","    \"\"\"\n","    try:\n","        final_psdf = ps.read_html(StringIO(final_table_html))[0]\n","        semifinal1_psdf = ps.read_html(StringIO(semifinal1_table_html))[0]\n","        semifinal2_psdf = ps.read_html(StringIO(semifinal2_table_html))[0]\n","    except Exception as e:\n","        logger.error(f\"Error reading HTML tables: {e}\")\n","        raise\n","\n","    logger.info(f\"Final table columns: {final_psdf.columns}\")\n","    logger.info(f\"Semi-final 1 table columns: {semifinal1_psdf.columns}\")\n","    logger.info(f\"Semi-final 2 table columns: {semifinal2_psdf.columns}\")\n","\n","    rename_mapping = {\n","        \"R/O\": \"running_order\",\n","        \"Country\": \"country\",\n","        \"Artist\": \"artist\", \n","        \"Song\": \"song\", \n","        \"Points\": \"points\",\n","        \"Place\": \"place\"\n","    }\n","    final_psdf = final_psdf.rename(columns=rename_mapping).reset_index(drop=True)\n","    semifinal1_psdf = semifinal1_psdf.rename(columns=rename_mapping).reset_index(drop=True)\n","    semifinal2_psdf = semifinal2_psdf.rename(columns=rename_mapping).reset_index(drop=True)\n","\n","    final_df = final_psdf.to_spark()\n","    semifinal1_df = semifinal1_psdf.to_spark()\n","    semifinal2_df = semifinal2_psdf.to_spark()\n","\n","    final_df.cache(); semifinal1_df.cache(); semifinal2_df.cache()\n","    final_df.count(); semifinal1_df.count(); semifinal2_df.count()\n","\n","    return final_df, semifinal1_df, semifinal2_df\n","\n","def combine_final_and_semi(final_df: DataFrame, semifinal1_df: DataFrame, semifinal2_df: DataFrame, year: int) -> DataFrame:\n","    \"\"\"\n","    Given the final and semi-final DataFrames, first union the semi-final results\n","    (while adding an indicator of the semi-final number), then join that union with the final DF.\n","    For countries appearing only in the semi-finals, append the rows.\n","    Finally, add a \"year\" column.\n","    \n","    Returns:\n","         The combined Spark DataFrame for that year.\n","    \"\"\"\n","    semifinal1_df = semifinal1_df.withColumn(\"semi_final\", F.lit(1))\n","    semifinal2_df = semifinal2_df.withColumn(\"semi_final\", F.lit(2))\n","    \n","    semi_df = semifinal1_df.unionByName(semifinal2_df)\n","    semi_df = semi_df.select(\n","        \"country\",\n","        \"artist\",\n","        \"song\",\n","        \"semi_final\",\n","        F.col(\"points\").alias(\"sf_points\"),\n","        F.col(\"place\").alias(\"sf_place\")\n","    ).cache()\n","    semi_df.count()\n","    \n","    final_with_semi = final_df.join(\n","        semi_df, on=\"country\", how=\"left\"\n","    ).select(\n","        final_df[\"country\"],\n","        final_df[\"artist\"],\n","        final_df[\"song\"],\n","        final_df[\"place\"],\n","        final_df[\"points\"],\n","        semi_df[\"semi_final\"],\n","        semi_df[\"sf_points\"],\n","        semi_df[\"sf_place\"]\n","    )\n","    \n","    semi_only = semi_df.join(final_df, on=\"country\", how=\"left_anti\") \\\n","        .withColumn(\"place\", F.lit(None).cast(T.StringType())) \\\n","        .withColumn(\"points\", F.lit(None).cast(T.StringType()))\n","    semi_only = semi_only.select(\n","        \"country\", \"artist\", \"song\", \"place\", \"points\", \"semi_final\", \"sf_points\", \"sf_place\"\n","    )\n","    \n","    result_df = final_with_semi.unionByName(semi_only)\n","    result_df = result_df.withColumn(\"year\", F.lit(year))\n","    logger.info(f\"Year {year}: Processed {result_df.count()} rows.\")\n","    return result_df\n","\n","def process_year(year: int, spark: SparkSession) -> DataFrame:\n","    \"\"\"\n","    Process the specified year by orchestrating:\n","        1. Fetching the Wikipedia page.\n","        2. Extracting the table HTML strings.\n","        3. Loading and standardizing the tables.\n","        4. Combining final with semi-final data.\n","    \n","    Returns:\n","         A Spark DataFrame (with a \"year\" column) for the given year.\n","    \"\"\"\n","    soup = fetch_year_page(year)\n","    final_html, semi1_html, semi2_html = extract_table_htmls(soup, year)\n","    final_df, semi1_df, semi2_df = load_and_standardize_tables(final_html, semi1_html, semi2_html)\n","    return combine_final_and_semi(final_df, semi1_df, semi2_df, year)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":31,"statement_ids":[31],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.9134596Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:38.2463772Z","execution_finish_time":"2025-04-28T21:00:38.5300089Z","parent_msg_id":"86662429-7a2e-42d7-9db0-d9388d026837"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 31, Finished, Available, Finished)"},"metadata":{}}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79891b42-0b5d-4444-9d91-8202d36141cb"},{"cell_type":"code","source":["def extract_participant_html(response_text: str, year: int, caption: str) -> str:\n","    \"\"\"\n","    Extracts the HTML content of the participants table from the given response text.\n","\n","    This function parses the provided HTML response text, searches for a table with the class\n","    \"wikitable\" that contains a caption indicating it is a participants table, and returns the\n","    HTML content of that table. If no such table is found, a ValueError is raised.\n","\n","    Args:\n","        response_text (str): The HTML response text to search within.\n","        year (int): The year associated with the request.\n","\n","    Returns:\n","        str: The HTML content of the participants table.\n","\n","    Raises:\n","        ValueError: If no participants table is found in the response text.\n","    \"\"\"\n","    soup = BeautifulSoup(response_text, \"html.parser\")\n","    participant_table_content: Optional[str] = None\n","    # Iterate over all wikitable tags to find one with a caption containing \"participants\"\n","    for table in soup.find_all(\"table\", class_=\"wikitable\"):\n","        caption_tag = table.find(\"caption\")\n","        if caption_tag:\n","            caption_text = caption_tag.get_text(strip=True).lower()\n","            if caption.casefold() in caption_text:\n","                participant_table_content = str(table)\n","                logger.info(f\"Year {year}: Participants table found with caption: {caption_text}\")\n","                break\n","    if not participant_table_content:\n","        logger.warning(f\"Year {year}: Participants table not found.\")\n","        raise ValueError(\n","            f\"Participants table not found for year {year}. Ensure the response contains a valid 'wikitable' with the appropriate caption.\"\n","        )\n","    return participant_table_content"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:36.9468573Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:38.5326713Z","execution_finish_time":"2025-04-28T21:00:38.8178511Z","parent_msg_id":"52f33264-fe7b-415a-a967-71e13eb7ce43"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 32, Finished, Available, Finished)"},"metadata":{}}],"execution_count":26,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c7b54a0-df56-4645-8f04-06f28a452946"},{"cell_type":"code","source":["def process_participants(year: int, spark: SparkSession, caption: str) -> DataFrame:\n","    \"\"\"\n","    For a year with no contest results yet (e.g., 2025), fetch the Wikipedia page section\n","    for \"Participating countries\", extract the table containing participants, and return a DataFrame.\n","    \n","    This DataFrame will contain the columns:\n","        country, artist, song,\n","    with placeholder null values for columns such as:\n","        place, points, semi_final, sf_points, sf_place,\n","    and includes a \"year\" column.\n","\n","    The data is converted to pandas dataframes so that the HTML parsing can be done with soup. \n","    Once completed it goes back to spark dataframes.\n","    \"\"\"\n","    # Use the \"#Participating_countries\" fragment so that we target that section.\n","    url = f\"https://en.wikipedia.org/wiki/Eurovision_Song_Contest_{year}\"\n","    headers_req = {\"User-Agent\": \"Mozilla/5.0\"}\n","    logger.info(f\"Year {year}: Fetching participants URL {url}\")\n","    try:\n","        response = requests.get(url, headers=headers_req)\n","        response.raise_for_status()\n","    except Exception as e:\n","        logger.error(f\"Year {year}: Error fetching participants URL: {e}\")\n","        raise\n","\n","    try:\n","        participant_table_html = extract_participant_html(response.text, year, caption)\n","        participants_psdf = ps.read_html(StringIO(participant_table_html))[0]\n","    except Exception as e:\n","        logger.error(f\"Year {year}: Error reading participants table: {e}\")\n","        raise\n","\n","    logger.info(f\"Year {year}: Participants table columns: {participants_psdf.columns}\")\n","\n","    # Rename columns if necessary. We expect at least \"Country\", \"Artist\" and \"Song\".\n","    rename_mapping = {\n","        \"Country\": \"country\",\n","        \"Artist\": \"artist\",\n","        \"Song\": \"song\"\n","    }\n","    participants_psdf = participants_psdf.rename(columns=rename_mapping).reset_index(drop=True)\n","\n","    for col_name in [\"country\", \"artist\", \"song\"]:\n","        if col_name not in participants_psdf.columns:\n","            logger.error(f\"Year {year}: Expected column {col_name} not found in the participants table.\")\n","            raise ValueError(f\"Expected column {col_name} not found for year {year}.\")\n","\n","    participants_df = participants_psdf.to_spark(index_col=None)\n","\n","    participants_df = participants_df.select(\"country\",\"artist\",\"song\")\n","    # Add placeholder columns (set to null) for contest result fields.\n","    participants_df = participants_df.withColumn(\"place\", F.lit(None).cast(T.StringType()))\n","    participants_df = participants_df.withColumn(\"points\", F.lit(None).cast(T.StringType()))\n","    participants_df = participants_df.withColumn(\"semi_final\", F.lit(None).cast(T.IntegerType()))\n","    participants_df = participants_df.withColumn(\"sf_points\", F.lit(None).cast(T.StringType()))\n","    participants_df = participants_df.withColumn(\"sf_place\", F.lit(None).cast(T.StringType()))\n","    \n","    # Replace \"TBA\" values in artist and song with null.\n","    participants_df = participants_df.withColumn(\n","        \"artist\",\n","        F.when(F.col(\"artist\").rlike(\"(?i)^TBA.*\"), None).otherwise(F.col(\"artist\"))\n","    ).withColumn(\n","        \"song\",\n","        F.when(F.col(\"song\").rlike(\"(?i)^TBA.*\"), None).otherwise(F.col(\"song\"))\n","    )\n","    \n","    # Add the year column.\n","    participants_df = participants_df.withColumn(\"year\", F.lit(year))\n","    \n","    return participants_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:37.0630235Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:38.8198592Z","execution_finish_time":"2025-04-28T21:00:39.1207804Z","parent_msg_id":"c91f45d8-0700-4648-8f39-07cff3275512"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 33, Finished, Available, Finished)"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2a180d38-2270-4d56-b06a-3a24f5d98888"},{"cell_type":"code","source":["# Read distinct years from the 'playlists' table (which contains a column 'Year').\n","try:\n","    playlists_df = spark.read.format(\"delta\").load(PLAYLISTS_TABLE)\n","    playlists_df = playlists_df.filter(\"Year <> 2020\") # Removed as event did not happen\n","    distinct_years = [int(row[\"Year\"]) for row in playlists_df.select(\"Year\").distinct().collect()]\n","    logger.info(f\"Found years from 'playlists' table: {distinct_years}\")\n","\n","    today = datetime.today()\n","    current_year = today.year\n","    if current_year in distinct_years and today <= datetime(current_year, 5, 17):\n","        distinct_years.remove(current_year)\n","        logger.info(f\"Current year {current_year} removed because today's date is before May 17.\")\n","except Exception as e:\n","    logger.error(f\"Error reading playlists table: {e}\")\n","    raise\n","        "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:37.166058Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:39.1228126Z","execution_finish_time":"2025-04-28T21:00:48.7606188Z","parent_msg_id":"570e8209-56a0-48d6-bb60-a3d9153573a8"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 34, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-04-28 21:00:47,190 - INFO - Found years from 'playlists' table: [2021, 2022, 2024, 2025, 2023]\n2025-04-28 21:00:47,191 - INFO - Current year 2025 removed because today's date is before May 17.\n"]}],"execution_count":28,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba0f8157-9bce-4ccc-a6ae-2709e9dbcc3b"},{"cell_type":"code","source":["results = []\n","\n","# Process multiple years in parallel using ThreadPoolExecutor\n","with concurrent.futures.ThreadPoolExecutor(max_workers=len(distinct_years)) as executor:\n","    future_to_year = {executor.submit(process_year, year, spark): year for year in distinct_years}\n","    for future in concurrent.futures.as_completed(future_to_year):\n","        yr = future_to_year[future]\n","        try:\n","            df_year = future.result()\n","            results.append(df_year)\n","            logger.info(f\"Year {yr} processed successfully.\")\n","        except Exception as e:\n","            logger.error(f\"Year {yr} generated an exception: {e}\")\n","            \n","if not results:\n","    message = \"No data processed for any year.\"\n","    logger.error(message)\n","    notebookutils.notebook.exit(message)\n","\n","# Union all yearly DataFrames\n","union_df = results[0]\n","for df in results[1:]:\n","    union_df = union_df.unionByName(df)\n","\n","results_2020 = process_participants(2020,spark,\"Eurovision Song Contest 2020 selected participants\")\n","union_df = union_df.unionByName(results_2020)\n","\n","if today <= datetime(current_year, 5, 17):\n","    results_2025 = process_participants(2025,spark,\"eurovision song contest 2025 participants\")\n","    union_df = union_df.unionByName(results_2025)\n","\n","union_df = union_df.withColumn(\"song\", F.regexp_replace(F.col(\"song\"), '^\"+|\"+$', ''))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:00:37.3282226Z","session_start_time":null,"execution_start_time":"2025-04-28T21:00:48.7628982Z","execution_finish_time":"2025-04-28T21:00:58.6382952Z","parent_msg_id":"a0a96829-b549-4e9b-984d-a7f6599c2d61"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 35, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-04-28 21:00:48,887 - INFO - Year 2021: Fetching URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2021#Contest_overview\n2025-04-28 21:00:48,889 - INFO - Year 2022: Fetching URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2022#Contest_overview\n2025-04-28 21:00:48,890 - INFO - Year 2024: Fetching URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2024#Contest_overview\n2025-04-28 21:00:48,891 - INFO - Year 2023: Fetching URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2023#Contest_overview\n2025-04-28 21:00:51,514 - INFO - Year 2021: Semi-final 1 table found with caption: results of the first semi-final of the eurovision song contest 2021[109]\n2025-04-28 21:00:51,552 - INFO - Year 2024: Semi-final 1 table found with caption: first semi-final of the eurovision song contest 2024[148]\n2025-04-28 21:00:51,600 - INFO - Year 2021: Semi-final 2 table found with caption: results of the second semi-final of the eurovision song contest 2021[115]\n2025-04-28 21:00:51,630 - INFO - Year 2024: Semi-final 2 table found with caption: second semi-final of the eurovision song contest 2024[153]\n2025-04-28 21:00:51,654 - INFO - Year 2022: Semi-final 1 table found with caption: results of the first semi-final of the eurovision song contest 2022[149]\n2025-04-28 21:00:51,659 - INFO - Year 2021: Final table found with caption: results of the final of the eurovision song contest 2021[126]\n2025-04-28 21:00:51,664 - INFO - Year 2024: Final table found with caption: final of the eurovision song contest 2024[164]\n2025-04-28 21:00:51,668 - INFO - Year 2022: Semi-final 2 table found with caption: results of the second semi-final of the eurovision song contest 2022[155]\n2025-04-28 21:00:51,679 - INFO - Year 2022: Final table found with caption: results of the final of the eurovision song contest 2022[157]\n2025-04-28 21:00:53,036 - INFO - Year 2023: Semi-final 1 table found with caption: results of the first semi-final of the eurovision song contest 2023[195]\n2025-04-28 21:00:53,040 - INFO - Year 2023: Semi-final 2 table found with caption: results of the second semi-final of the eurovision song contest 2023[202]\n2025-04-28 21:00:53,045 - INFO - Year 2023: Final table found with caption: results of the final of the eurovision song contest 2023[205]\n2025-04-28 21:00:53,240 - INFO - Final table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,243 - INFO - Semi-final 1 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,244 - INFO - Semi-final 2 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,268 - INFO - Final table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,270 - INFO - Semi-final 1 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,272 - INFO - Semi-final 2 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,282 - INFO - Final table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,285 - INFO - Final table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,285 - INFO - Semi-final 1 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,289 - INFO - Semi-final 1 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,290 - INFO - Semi-final 2 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n2025-04-28 21:00:53,292 - INFO - Semi-final 2 table columns: Index(['R/O', 'Country', 'Artist', 'Song', 'Points', 'Place'], dtype='object')\n/opt/spark/python/lib/pyspark.zip/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n2025-04-28 21:00:55,617 - INFO - Year 2022: Processed 40 rows.\n2025-04-28 21:00:55,618 - INFO - Year 2022 processed successfully.\n2025-04-28 21:00:55,618 - INFO - Year 2024: Processed 37 rows.\n2025-04-28 21:00:55,620 - INFO - Year 2024 processed successfully.\n2025-04-28 21:00:55,630 - INFO - Year 2021: Processed 39 rows.\n2025-04-28 21:00:55,631 - INFO - Year 2021 processed successfully.\n2025-04-28 21:00:55,652 - INFO - Year 2023: Processed 37 rows.\n2025-04-28 21:00:55,652 - INFO - Year 2023 processed successfully.\n2025-04-28 21:00:55,726 - INFO - Year 2020: Fetching participants URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2020\n2025-04-28 21:00:56,156 - INFO - Year 2020: Participants table found with caption: eurovision song contest 2020 selected participants[40][42]\n2025-04-28 21:00:56,192 - INFO - Year 2020: Participants table columns: Index(['Country', 'Broadcaster', 'Song', 'Artist', 'Songwriter(s)',\n       'Language'],\n      dtype='object')\n/opt/spark/python/lib/pyspark.zip/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n2025-04-28 21:00:56,312 - INFO - Year 2025: Fetching participants URL https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2025\n2025-04-28 21:00:56,708 - INFO - Year 2025: Participants table found with caption: eurovision song contest 2025 participants[8][54]\n2025-04-28 21:00:56,740 - INFO - Year 2025: Participants table columns: Index(['Country', 'Broadcaster', 'Artist', 'Song', 'Language', 'Songwriter(s)',\n       'Ref.'],\n      dtype='object')\n/opt/spark/python/lib/pyspark.zip/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n"]}],"execution_count":29,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"412f88e2-ae3e-46f2-b1a2-3719f034727f"},{"cell_type":"markdown","source":["### Merge data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b466a2e-618b-48fe-932f-65bf3e8a8652"},{"cell_type":"code","source":["def check_table_and_create_if_not_exists(table_path: str, schema: T.StructType) -> None:\n","    directory, table_name = table_path.rsplit('/', 1)\n","    all_databases = spark.catalog.listDatabases()\n","    database = [t for t in all_databases if t.locationUri == directory][0].name\n","    logger.debug(f\"Checking table {database}.{table_name}\")\n","    if not spark.catalog.tableExists(f\"`{database}`.`{table_name}`\"):\n","        logger.info(\"Table doesn't exists, starting to create\")\n","        df = spark.createDataFrame([], schema)\n","        df.write.format(\"delta\").save(table_path)\n","        logger.info(\"Table creation finished\")\n","    else:\n","        logger.debug(\"Table already exists\")\n","    return"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":39,"statement_ids":[39],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:01:21.3423839Z","session_start_time":null,"execution_start_time":"2025-04-28T21:01:21.3435762Z","execution_finish_time":"2025-04-28T21:01:21.6183796Z","parent_msg_id":"6212fea8-e374-4661-8995-231316644d5b"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 39, Finished, Available, Finished)"},"metadata":{}}],"execution_count":33,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e6304797-48d3-4ea7-a652-93f427e11554"},{"cell_type":"code","source":["def merge_results_data(union_df: DataFrame, table_path: str) -> None:\n","    \"\"\"\n","    Merge the comments data into the Delta table\n","    \n","    Args:\n","        union_df: The DataFrame containing the new video data and details.\n","        table_path: The Delta table ABFS path to merge into.\n","    \"\"\"\n","    try:\n","        new_columns = [(\"_modified_date\", T.TimestampType(), True), (\"_created_date\", T.TimestampType(), True)]\n","        new_schema = union_df.schema\n","        for col_name, col_type, nullable in new_columns:\n","            new_schema = new_schema.add(col_name, col_type, nullable)\n","        check_table_and_create_if_not_exists(table_path, new_schema)\n","\n","        target_table = DeltaTable.forPath(spark, table_path)\n","        logger.info(\"Merging data started\")\n","        target_table.alias(\"target\").merge(\n","            union_df.alias(\"source\"),\n","            \"\"\"\n","            target.country = source.country \n","            AND target.song = source.song \n","            AND target.year = source.year\n","            \"\"\"\n","        ).whenMatchedUpdate(\n","            condition=\"\"\"\n","            target.artist <> source.artist \n","            OR target.place <> source.place \n","            OR target.points <> source.points \n","            OR target.semi_final <> source.semi_final \n","            OR target.sf_points <> source.sf_points \n","            OR target.sf_place <> source.sf_place\n","            \"\"\",\n","            set={\n","                \"artist\": \"source.artist\",\n","                \"place\": \"source.place\",\n","                \"points\": \"source.points\",\n","                \"semi_final\": \"source.semi_final\",\n","                \"sf_points\": \"source.sf_points\",\n","                \"sf_place\": \"source.sf_place\",\n","                \"_modified_date\": \"current_timestamp()\"\n","            }\n","        ).whenNotMatchedInsert(\n","            values={\n","                \"country\": \"source.country\",\n","                \"artist\": \"source.artist\",\n","                \"song\": \"source.song\",\n","                \"place\": \"source.place\",\n","                \"points\": \"source.points\",\n","                \"semi_final\": \"source.semi_final\",\n","                \"sf_points\": \"source.sf_points\",\n","                \"sf_place\": \"source.sf_place\",\n","                \"year\": \"source.year\",\n","                \"_created_date\": \"current_timestamp()\",\n","                \"_modified_date\": \"current_timestamp()\"\n","            }\n","        ).execute()\n","        logger.info(\"Merging data finished\")\n","        lastCommit = target_table.history(1).collect()[0]\n","        metrics = lastCommit[\"operationMetrics\"] \n","\n","        numInserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n","        numUpdated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n","        numDeleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n","\n","        logger.info(f\"Rows inserted: {numInserted}\")\n","        logger.info(f\"Rows updated: {numUpdated}\")\n","        logger.info(f\"Rows deleted: {numDeleted}\")\n","        try:\n","            logger.info(\"Start optimize\")\n","            target_table.optimize().executeCompaction()\n","            logger.info(\"Finished optimize\")\n","        except Exception as e:\n","            logger.error(\"Failed to optimize\")\n","            raise\n","\n","    except Exception as e:\n","        logger.exception(f\"Exception details: {str(e)}\")\n","        raise\n","        \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":41,"statement_ids":[41],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:02:03.2711148Z","session_start_time":null,"execution_start_time":"2025-04-28T21:02:03.2724811Z","execution_finish_time":"2025-04-28T21:02:03.554689Z","parent_msg_id":"8fce9149-73e0-48f4-b2df-2dcbe0cee0e6"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 41, Finished, Available, Finished)"},"metadata":{}}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"186524ce-c08e-4542-8284-bb535549315d"},{"cell_type":"code","source":["merge_results_data(union_df,RESULTS_TABLE)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":42,"statement_ids":[42],"state":"finished","livy_statement_state":"available","session_id":"03e79284-5638-40ba-9a83-6ef57b8fc077","normalized_state":"finished","queued_time":"2025-04-28T21:02:06.3127501Z","session_start_time":null,"execution_start_time":"2025-04-28T21:02:06.3138001Z","execution_finish_time":"2025-04-28T21:02:22.6436551Z","parent_msg_id":"a55323fd-1255-4dfa-8349-051a573b444c"},"text/plain":"StatementMeta(, 03e79284-5638-40ba-9a83-6ef57b8fc077, 42, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-04-28 21:02:06,922 - INFO - Merging data started\n2025-04-28 21:02:18,337 - INFO - Merging data finished\n2025-04-28 21:02:19,009 - INFO - Rows inserted: 5\n2025-04-28 21:02:19,009 - INFO - Rows updated: 1\n2025-04-28 21:02:19,010 - INFO - Rows deleted: 0\n2025-04-28 21:02:19,011 - INFO - Start optimize\n2025-04-28 21:02:21,694 - INFO - Finished optimize\n"]}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09c73694-8fd2-40be-95b3-d0608e3a5e8e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"77b89b44-1bcf-42fa-a9ac-7d0593123d3d","known_lakehouses":[{"id":"77b89b44-1bcf-42fa-a9ac-7d0593123d3d"}],"default_lakehouse_name":"Raw","default_lakehouse_workspace_id":"fd12376e-2797-4027-bb8e-42a3a8228a70"},"environment":{}}},"nbformat":4,"nbformat_minor":5}